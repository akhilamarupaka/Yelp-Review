# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QjC2Wq8lc3zR-SWEchCPxySTeryGCj7o
"""

pip install transformers datasets peft accelerate bitsandbytes evaluate

from datasets import load_dataset
from transformers import AutoTokenizer

# Load Yelp dataset
dataset = load_dataset("yelp_review_full")
train_data = dataset["train"].shuffle(seed=42).select(range(2000))  # small subset
test_data = dataset["test"].select(range(500))

# Load tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize
def preprocess(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

train_data = train_data.map(preprocess, batched=True)
test_data = test_data.map(preprocess, batched=True)

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)

# Evaluate without training
from evaluate import load
import numpy as np

metric = load("accuracy")

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    return metric.compute(predictions=preds, references=p.label_ids)

args = TrainingArguments(
    output_dir="./results",
    per_device_eval_batch_size=16,
)

trainer = Trainer(
    model=base_model,
    args=args,
    eval_dataset=test_data,
    compute_metrics=compute_metrics,
)

base_metrics = trainer.evaluate()
print("Base model accuracy:", base_metrics["eval_accuracy"])

from peft import get_peft_model, LoraConfig, TaskType
import torch

# Load model for LoRA
lora_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)

# Define LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS,
)

# Wrap with PEFT
lora_model = get_peft_model(lora_model, lora_config)

# Training args
lora_args = TrainingArguments(
    output_dir="./lora-results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    learning_rate=2e-4,
)

# Trainer
lora_trainer = Trainer(
    model=lora_model,
    args=lora_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    compute_metrics=compute_metrics,
)

lora_trainer.train()
lora_eval = lora_trainer.evaluate()
print("LoRA model accuracy:", lora_eval["eval_accuracy"])

from transformers import BitsAndBytesConfig

# Quantization config (8-bit)
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=True,
)

# Load quantized model
qlora_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=5,
    quantization_config=bnb_config,
    device_map="auto"
)

# Apply LoRA on top of quantized model
qlora_model = get_peft_model(qlora_model, lora_config)

# Same trainer setup
qlora_trainer = Trainer(
    model=qlora_model,
    args=lora_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    compute_metrics=compute_metrics,
)

qlora_trainer.train()
qlora_eval = qlora_trainer.evaluate()
print("QLoRA model accuracy:", qlora_eval["eval_accuracy"])